# Example of using the generic pipeline for binary classification.
#
# Sample data are from this census dataset:
# https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names
#
# The goal is to predict whether a person makes over $50k in yearly income.
#

job_name : "Generic Pipeline"

# Settings as to where the data goes
training_data_version : 1
model_version: "a"

# Which model config to use
model_type : "linear"
model_config : ${model_type}"_model_config"
demo_table : "pricing.income_training_raw"

# Where to dump data; change prefix based on your HDFS directory layout
prefix : "hdfs://airfs-silver/user/"${USER}"/demo_pipeline"
training_data : ${prefix}"/training_data"${training_data_version}
eval_output : ${prefix}"/eval_output/"${training_data_version}_${model_version}".eval"
model_name : ${prefix}"/model/"${training_data_version}_${model_version}_${model_type}".model"
model_dump : ${model_name}".tsv"
calibrated_model_name : ${prefix}"/calibrated_model/"${training_data_version}_${model_version}".model"
scoring_output : ${prefix}"/scores/"${training_data_version}

train_subsample : 0.1
eval_subsample : 0.1

# The convention is to name features FAMILY_SHORTNAME
# The reason for feature families is so we can apply transforms to multiple, related features
# at a time.
#
# By default two families are created:
# (1) BIAS, B and
# (2) MISS, COLUMN_NAME for missing features
#
# The exception is a special feature named "LABEL" which has to be a float.
#
# For the example query, we use two families, "i" for numeric features and "s" for string ones.
generic_hive_query : """
  select
    if(LENGTH(label)=5, -1, 1) as LABEL,
    age as i_age,
    workclass as s_workclass, fnlwgt as s_fnlwgt,
    education as s_education,
    education_num as i_educationnum,
    marital_status as s_marital,
    occupation as s_occupation,
    relationship as s_relationship,
    race as s_race, sex as s_sex,
    capital_gain,
    capital_loss,
    hours_per_week as i_hours,
    native_country as s_country
"""

debug_example {
  hive_query : ${generic_hive_query}" from "${demo_table}
  count : 10
}

debug_transforms {
  hive_query : ${generic_hive_query}" from "${demo_table}
  model_config : ${model_config}
  count : 3
}

make_training {
  hive_query : ${generic_hive_query}" from "${demo_table}
  num_shards : 20
  output : ${training_data}
}

train_model {
  input : ${training_data}
  subsample : ${train_subsample}
  model_config : ${model_config}
}

eval_model {
  input : ${training_data}
  subsample : ${eval_subsample}
  bins : 5
  model_config : ${model_config}
  is_probability : false
  is_regression : false
  metric_to_maximize : "!HOLD_F1"
  model_name : ${model_name}
}

calibrate_model {
  model_config : ${model_config}
  model_name : ${model_name}
  calibrated_model_output : ${calibrated_model_name}
  input: ${training_data}
  learning_rate : 0.01
  rate_decay : 0.95
  iterations : 100
  num_bags : 10
  tolerance : 0.01
  subsample : 0.5
}

eval_model_calibrated {
  input : ${training_data}
  subsample : ${eval_subsample}
  bins : 5
  model_config : ${model_config}
  is_probability : true
  metric_to_maximize : "!HOLD_F1"
  model_name : ${calibrated_model_name}
}

dump_model {
  model_name : ${model_name}
  model_dump : ${model_dump}
}

score_table {
  model_config : ${model_config}
  model_name : ${calibrated_model_name}
  hive_query : ${generic_hive_query}", id as UNIQUE_ID from pricing.income_training_ids"
  num_shards : 20
  output : ${scoring_output}
}

debug_score_table {
  model_config : ${model_config}
  model_name : ${calibrated_model_name}
  hive_query : ${generic_hive_query}", id as UNIQUE_ID from pricing.income_training_ids"
  count : 10
}

identity_transform {
  transform : list
  transforms : [ ]
}

quantize_capital {
  transform : multiscale_quantize
  field1 : "capital"
  output : "capital"
  buckets : [500, 2000]
}

move_age {
  transform : multiscale_move_float_to_string
  field1 : "i"
  keys: [
    "age"
  ]
  output : "A"
  buckets : [5.0]
}

move_hours {
  transform : multiscale_move_float_to_string
  field1 : "i"
  keys: [
    "hours"
  ]
  output : "A"
  buckets : [5.0]
}

move_edu {
  transform : multiscale_move_float_to_string
  field1 : "i"
  keys: [
    "educationnum"
  ]
  output : "A"
  buckets : [3.0]
}

combined_transform {
  transform : list
  transforms : [
    "quantize_capital"
    "move_age"
    "move_hours"
    "move_edu"
  ]
}

# Config for a linear product quantization model. This model is heavily
# dependent upon feature transforms being set up correctly as it only
# handles discrete (i.e., string) features and all float features
# have to be quantized. This is so that we can debug the model easily.
linear_model_config {
  trainer : "linear"
  prior : [
    "BIAS,B,0.0", # Set the priors using feature family, feature name, initial weight
  ]
  model_output : ${model_name}
  rank_key : "LABEL"
  loss : "hinge"
  margin : 1.0
  rank_threshold : 0.5
  dropout : 0.1
  learning_rate : 0.01
  num_bags : 10
  iterations : 10
  lambda : 0.001
  lambda2 : 0.01
  context_transform : identity_transform
  item_transform : identity_transform
  combined_transform : combined_transform
}

# Forest of trees model. Less feature engineering needed but less debuggable.
# Use this if you just want something fast and easy to use.
forest_model_config {
  trainer : "forest"
  model_output : ${model_name}
  rank_key : "LABEL"
  rank_threshold : 0.5
  # How many trees
  num_trees : 100
  # How many samples to use per tree
  num_candidates : 100000
  # Max depth of a tree
  max_depth : 6
  # Minimum number of samples in each leaf
  min_leaf_items : 3000
  # How many times per split to search for an optimal split
  num_tries : 100
  context_transform : identity_transform
  item_transform : identity_transform
  combined_transform : identity_transform
}

param_search {
  # Model type
  model_config : ${model_config}
  # Optimization target
  metric_to_maximize : "!HOLD_F1"
  # Path and prefix used to store all models
  model_name_prefix : ${model_name}
  # Max number of trials in parameter search, each round has one training and one eval
  max_round: 8
  # Strategies to conduct parameter search. "guided" will take all provide values and
  # cross join them. "grid" will take an optional min (first val) and an optional max
  # (second val) and generate equal splits for each parameter based on max_round.
  search_strategy: "guided"
  # List of parameters to tune (defined in model_config). In "guided" stategy, provide
  # all parameters you want to try. In "grid" strategy, provide two optional values:
  # first is a min (default 0), second is max (default 1).
  param_to_tune: [
    {
      name: "lambda"
      val: [0.001, 0.01, 0.1]
    }
    {
      name: "lambda2"
      val: [0.01, 0.06, 0.1]
    }
  ]
  # Example for grid strategy
  #  param_to_tune: [
  #    {
  #      name: "lambda"
  #      val: [0.001, 0.1]
  #    }
  #    {
  #      name: "lambda2"
  #      val: [0.01, 0.1]
  #    }
  #  ]

  # (Optional) Where you want to store the results. If omitted, results will not be stored.
  # Results will be displayed in terminal log regardlessly.
  output: ${prefix}"/paramsearch/"${training_data_version}_${model_version}_${model_type}".txt"
  # (Optional) Where you want to copy the best model to.
  best_model_output: ${model_name}
}
